{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pruning.tracer import GroupTracer\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from pruning.utils import fuse_batchnorm, run_model\n",
    "from pruning.feature_merging.importance import GroupNormImportance, FPGMImportance\n",
    "import torch\n",
    "import copy\n",
    "import torch_pruning\n",
    "\n",
    "model = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1).eval()\n",
    "fuse_batchnorm(model)\n",
    "original_model = copy.deepcopy(model)\n",
    "input_example = torch.rand((1, 3, 224, 224))\n",
    "output_initial = run_model(model, input_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(run_model(model, input_example).argmax() - output_initial.argmax()).abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "ignored_dims = {\n",
    "    'fc.weight': [0],\n",
    "    'fc.bias': [0]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = GroupTracer(model, ignored_dims).build_groups(input_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_fn = FPGMImportance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight\n",
      "conv1.bias\n",
      "layer1.0.conv1.weight\n",
      "layer1.0.conv2.weight\n",
      "layer1.0.conv2.bias\n",
      "layer1.1.conv1.weight\n",
      "layer1.1.conv2.weight\n",
      "layer1.1.conv2.bias\n",
      "layer2.0.conv1.weight\n",
      "layer2.0.downsample.0.weight\n"
     ]
    }
   ],
   "source": [
    "imp = imp_fn(groups[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8322, 0.9324, 0.7254, 1.1597, 0.8198, 1.3088, 1.1292, 1.2364, 1.1167,\n",
       "        0.9771, 0.7395, 1.4061, 1.7708, 0.8550, 0.8431, 0.8932, 0.8896, 1.2863,\n",
       "        1.0927, 0.7480, 1.0713, 0.8311, 0.8710, 0.8811, 0.8360, 0.7671, 0.7554,\n",
       "        0.8392, 1.4318, 0.7714, 0.8671, 0.8371, 1.1592, 0.9691, 0.7886, 0.7745,\n",
       "        1.1983, 0.8831, 1.2548, 0.9161, 1.5906, 0.7302, 0.7808, 0.9070, 0.7817,\n",
       "        0.9813, 1.0839, 0.7902, 0.9429, 1.7601, 1.2450, 0.8054, 1.6630, 0.7980,\n",
       "        1.0298, 0.7596, 0.7984, 0.7913, 0.9340, 0.8354, 0.7648, 1.0479, 1.0205,\n",
       "        1.6925])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch_pruning.pruner.importance import GroupNormImportance as GroupNormImportance_manual, FPGMImportance as FPGMImportance_manual\n",
    "\n",
    "imp_fn_manual = FPGMImportance_manual()\n",
    "imp_fn_manual(gr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "\n",
      "--------------------------------\n",
      "          Pruning Group\n",
      "--------------------------------\n",
      "[0] prune_out_channels on layer4.1.conv2 (Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))) => prune_out_channels on layer4.1.conv2 (Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))), #idxs=512\n",
      "[1] prune_out_channels on layer4.1.conv2 (Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))) => prune_out_channels on _ElementWiseOp_4(AddBackward0), #idxs=512\n",
      "[2] prune_out_channels on _ElementWiseOp_4(AddBackward0) => prune_out_channels on _ElementWiseOp_5(ReluBackward0), #idxs=512\n",
      "[3] prune_out_channels on _ElementWiseOp_4(AddBackward0) => prune_out_channels on _ElementWiseOp_3(ReluBackward0), #idxs=512\n",
      "[4] prune_out_channels on _ElementWiseOp_3(ReluBackward0) => prune_out_channels on _ElementWiseOp_2(MeanBackward1), #idxs=512\n",
      "[5] prune_out_channels on _ElementWiseOp_2(MeanBackward1) => prune_out_channels on _Reshape_0(), #idxs=512\n",
      "[6] prune_out_channels on _Reshape_0() => prune_in_channels on fc (Linear(in_features=512, out_features=1000, bias=True)), #idxs=512\n",
      "[7] prune_in_channels on fc (Linear(in_features=512, out_features=1000, bias=True)) => prune_out_channels on _ElementWiseOp_1(TBackward0), #idxs=512\n",
      "[8] prune_out_channels on _ElementWiseOp_5(ReluBackward0) => prune_out_channels on _ElementWiseOp_6(AddBackward0), #idxs=512\n",
      "[9] prune_out_channels on _ElementWiseOp_5(ReluBackward0) => prune_in_channels on layer4.1.conv1 (Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))), #idxs=512\n",
      "[10] prune_out_channels on _ElementWiseOp_6(AddBackward0) => prune_out_channels on layer4.0.conv2 (Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))), #idxs=512\n",
      "[11] prune_out_channels on _ElementWiseOp_6(AddBackward0) => prune_out_channels on layer4.0.downsample.0 (Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2))), #idxs=512\n",
      "--------------------------------\n",
      "\n",
      "0 3\n",
      "1\n",
      "\n",
      "--------------------------------\n",
      "          Pruning Group\n",
      "--------------------------------\n",
      "[0] prune_out_channels on layer3.1.conv2 (Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))) => prune_out_channels on layer3.1.conv2 (Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))), #idxs=256\n",
      "[1] prune_out_channels on layer3.1.conv2 (Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))) => prune_out_channels on _ElementWiseOp_8(AddBackward0), #idxs=256\n",
      "[2] prune_out_channels on _ElementWiseOp_8(AddBackward0) => prune_out_channels on _ElementWiseOp_9(ReluBackward0), #idxs=256\n",
      "[3] prune_out_channels on _ElementWiseOp_8(AddBackward0) => prune_out_channels on _ElementWiseOp_7(ReluBackward0), #idxs=256\n",
      "[4] prune_out_channels on _ElementWiseOp_7(ReluBackward0) => prune_in_channels on layer4.0.downsample.0 (Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2))), #idxs=256\n",
      "[5] prune_out_channels on _ElementWiseOp_7(ReluBackward0) => prune_in_channels on layer4.0.conv1 (Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))), #idxs=256\n",
      "[6] prune_out_channels on _ElementWiseOp_9(ReluBackward0) => prune_out_channels on _ElementWiseOp_10(AddBackward0), #idxs=256\n",
      "[7] prune_out_channels on _ElementWiseOp_9(ReluBackward0) => prune_in_channels on layer3.1.conv1 (Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))), #idxs=256\n",
      "[8] prune_out_channels on _ElementWiseOp_10(AddBackward0) => prune_out_channels on layer3.0.conv2 (Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))), #idxs=256\n",
      "[9] prune_out_channels on _ElementWiseOp_10(AddBackward0) => prune_out_channels on layer3.0.downsample.0 (Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))), #idxs=256\n",
      "--------------------------------\n",
      "\n",
      "1 3\n",
      "2\n",
      "\n",
      "--------------------------------\n",
      "          Pruning Group\n",
      "--------------------------------\n",
      "[0] prune_out_channels on layer2.1.conv2 (Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))) => prune_out_channels on layer2.1.conv2 (Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))), #idxs=128\n",
      "[1] prune_out_channels on layer2.1.conv2 (Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))) => prune_out_channels on _ElementWiseOp_12(AddBackward0), #idxs=128\n",
      "[2] prune_out_channels on _ElementWiseOp_12(AddBackward0) => prune_out_channels on _ElementWiseOp_13(ReluBackward0), #idxs=128\n",
      "[3] prune_out_channels on _ElementWiseOp_12(AddBackward0) => prune_out_channels on _ElementWiseOp_11(ReluBackward0), #idxs=128\n",
      "[4] prune_out_channels on _ElementWiseOp_11(ReluBackward0) => prune_in_channels on layer3.0.downsample.0 (Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))), #idxs=128\n",
      "[5] prune_out_channels on _ElementWiseOp_11(ReluBackward0) => prune_in_channels on layer3.0.conv1 (Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))), #idxs=128\n",
      "[6] prune_out_channels on _ElementWiseOp_13(ReluBackward0) => prune_out_channels on _ElementWiseOp_14(AddBackward0), #idxs=128\n",
      "[7] prune_out_channels on _ElementWiseOp_13(ReluBackward0) => prune_in_channels on layer2.1.conv1 (Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))), #idxs=128\n",
      "[8] prune_out_channels on _ElementWiseOp_14(AddBackward0) => prune_out_channels on layer2.0.conv2 (Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))), #idxs=128\n",
      "[9] prune_out_channels on _ElementWiseOp_14(AddBackward0) => prune_out_channels on layer2.0.downsample.0 (Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))), #idxs=128\n",
      "--------------------------------\n",
      "\n",
      "2 3\n",
      "3\n",
      "\n",
      "--------------------------------\n",
      "          Pruning Group\n",
      "--------------------------------\n",
      "[0] prune_out_channels on layer1.1.conv2 (Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))) => prune_out_channels on layer1.1.conv2 (Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))), #idxs=64\n",
      "[1] prune_out_channels on layer1.1.conv2 (Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))) => prune_out_channels on _ElementWiseOp_16(AddBackward0), #idxs=64\n",
      "[2] prune_out_channels on _ElementWiseOp_16(AddBackward0) => prune_out_channels on _ElementWiseOp_17(ReluBackward0), #idxs=64\n",
      "[3] prune_out_channels on _ElementWiseOp_16(AddBackward0) => prune_out_channels on _ElementWiseOp_15(ReluBackward0), #idxs=64\n",
      "[4] prune_out_channels on _ElementWiseOp_15(ReluBackward0) => prune_in_channels on layer2.0.downsample.0 (Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))), #idxs=64\n",
      "[5] prune_out_channels on _ElementWiseOp_15(ReluBackward0) => prune_in_channels on layer2.0.conv1 (Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))), #idxs=64\n",
      "[6] prune_out_channels on _ElementWiseOp_17(ReluBackward0) => prune_out_channels on _ElementWiseOp_18(AddBackward0), #idxs=64\n",
      "[7] prune_out_channels on _ElementWiseOp_17(ReluBackward0) => prune_in_channels on layer1.1.conv1 (Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))), #idxs=64\n",
      "[8] prune_out_channels on _ElementWiseOp_18(AddBackward0) => prune_out_channels on layer1.0.conv2 (Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))), #idxs=64\n",
      "[9] prune_out_channels on _ElementWiseOp_18(AddBackward0) => prune_out_channels on _ElementWiseOp_19(MaxPool2DWithIndicesBackward0), #idxs=64\n",
      "[10] prune_out_channels on _ElementWiseOp_19(MaxPool2DWithIndicesBackward0) => prune_out_channels on _ElementWiseOp_20(ReluBackward0), #idxs=64\n",
      "[11] prune_out_channels on _ElementWiseOp_19(MaxPool2DWithIndicesBackward0) => prune_in_channels on layer1.0.conv1 (Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))), #idxs=64\n",
      "[12] prune_out_channels on _ElementWiseOp_20(ReluBackward0) => prune_out_channels on conv1 (Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))), #idxs=64\n",
      "--------------------------------\n",
      "\n",
      "3 3\n"
     ]
    }
   ],
   "source": [
    "DG = torch_pruning.DependencyGraph()\n",
    "DG.build_dependency(model, example_inputs=input_example)\n",
    "ignored_layers = ['fc']\n",
    "\n",
    "groups_tp = DG.get_all_groups(ignored_layers=[model.get_submodule(l) for l in ignored_layers])\n",
    "\n",
    "idx_ = 3\n",
    "for i, gr in enumerate(groups_tp):\n",
    "    print(i)\n",
    "    print(gr)\n",
    "    print(i, idx_)\n",
    "    if i == idx_:\n",
    "        break\n",
    "\n",
    "    # if i == idx:\n",
    "    #     print(c)\n",
    "    #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_weights = torch.zeros((32, 64))\n",
    "important_indices = imp.argsort(descending=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_weights[:, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(dummy_important_indices, torch.Tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class FeatureMergingMLP(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels, important_indices=None):\n",
    "        super(FeatureMergingMLP, self).__init__()\n",
    "        self.output_channels = output_channels\n",
    "        self.fc = nn.Linear(input_channels, self.output_channels, bias=False)\n",
    "        self.important_indices = important_indices\n",
    "        if isinstance(self.important_indices, (torch.Tensor, list)):\n",
    "            self.important_indices = self.important_indices[:output_channels]\n",
    "        # TODO: CHECK HOW IT WORKS IN TP ()\n",
    "        self._initialize_weights()\n",
    "\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        if isinstance(self.important_indices, (torch.Tensor, list)):\n",
    "            weight_init = torch.zeros_like(self.fc.weight)\n",
    "            for idx, preserve_idx in enumerate(self.important_indices):\n",
    "                weight_init[idx, preserve_idx] = 1.0\n",
    "        else:\n",
    "            weight_init = torch.ones_like(self.fc.weight)\n",
    "        \n",
    "        self.fc.weight.data.copy_(weight_init)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "class FeatureMerging(nn.Module):\n",
    "    def __init__(self, size, new_size, dim, important_indices=None):\n",
    "        super(FeatureMerging, self).__init__()\n",
    "        self.size = size\n",
    "        self.new_size = new_size\n",
    "        self.dim = dim\n",
    "        self.mlp = FeatureMergingMLP(size, new_size, important_indices)\n",
    "\n",
    "    def forward(self, x):\n",
    "        reshaped_x = (\n",
    "            x.transpose(self.dim, 0)  # transpose to make pruning dim the first dimension\n",
    "            .reshape(self.size, -1)  # flatten all dimensions except the pruning idm\n",
    "            .permute(1, 0)           # switch the first and second dimensions to fit MLP\n",
    "        )\n",
    "\n",
    "        processed_weights = self.mlp(reshaped_x)\n",
    "        final_shape = list(x.shape)\n",
    "        final_shape[self.dim] = self.new_size\n",
    "        return processed_weights.permute(1, 0).reshape(final_shape)\n",
    "\n",
    "\n",
    "size = 64\n",
    "new_size = 32\n",
    "dim = 0\n",
    "\n",
    "old_tensor = torch.randint(10, (size, 512, 3, 3))\n",
    "dummy_important_indices = [0, 2, 1]\n",
    "feature_merging = FeatureMerging(size, new_size, dim, dummy_important_indices)\n",
    "\n",
    "assert torch.allclose(\n",
    "    old_tensor[dummy_important_indices].float(), \n",
    "    feature_merging(old_tensor.float())[:len(dummy_important_indices)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight: 0\n",
      "conv1.bias: 0\n",
      "layer1.0.conv1.weight: 1\n",
      "layer1.0.conv2.weight: 0\n",
      "layer1.0.conv2.bias: 0\n",
      "layer1.1.conv1.weight: 1\n",
      "layer1.1.conv2.weight: 0\n",
      "layer1.1.conv2.bias: 0\n",
      "layer2.0.conv1.weight: 1\n",
      "layer2.0.downsample.0.weight: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for group in groups:\n",
    "    size = group.size\n",
    "    print(group)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.utils.parametrize as parametrize\n",
    "import copy\n",
    "\n",
    "model = copy.deepcopy(original_model)\n",
    "\n",
    "new_size = 64\n",
    "for param in group.params:\n",
    "    module_name, wb = param['name'].rsplit('.', 1)\n",
    "    submodule = model.get_submodule(module_name)\n",
    "    important_indices = important_indices[:new_size]\n",
    "    if wb == 'weight':\n",
    "        parametrize.register_parametrization(\n",
    "            submodule, \n",
    "            \"weight\", \n",
    "            FeatureMerging(group.size, new_size, param['dim']), \n",
    "            unsafe=True\n",
    "        )\n",
    "    elif wb == 'bias':\n",
    "        submodule.bias.data = submodule.bias.data[important_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(428)"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(run_model(model, input_example).argmax() - output_initial.argmax()).abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "check = FeatureMerging(group.size, new_size, 0, important_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_weights = model.conv1.parametrizations.weight.original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([12, 49, 63, 52, 40, 28, 11,  5, 17, 38, 50,  7, 36,  3, 32,  6,  8, 18,\n",
       "        46, 20, 61, 54, 62, 45,  9, 33, 48, 58,  1, 39, 43, 15, 16, 37, 23, 22,\n",
       "        30, 13, 14, 27, 31, 24, 59,  0, 21,  4, 51, 56, 53, 57, 47, 34, 44, 42,\n",
       "        35, 29, 25, 60, 55, 26, 19, 10, 41,  2])"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "important_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.allclose(\n",
    "    original_weights[important_indices[:new_size]].float(), \n",
    "    model.conv1.weight\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 3, 7, 7])"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([147, 64])\n",
      "torch.Size([64, 3, 7, 7]) [32, 3, 7, 7]\n",
      "torch.Size([147, 32])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 7, 7])"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_w = check(original_weights)\n",
    "new_w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.allclose(\n",
    "    original_weights[important_indices],\n",
    "    new_w[:len(important_indices)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.allclose(\n",
    "    original_weights[0],\n",
    "    new_w[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3, 7, 7])"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_w[:len(important_indices)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 5]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "important_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bias\n",
      "parametrizations.weight.original\n",
      "parametrizations.weight.0.mlp.fc.weight\n",
      "parametrizations.weight.1.mlp.fc.weight\n",
      "parametrizations.weight.2.mlp.fc.weight\n"
     ]
    }
   ],
   "source": [
    "for name, p in model.conv1.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch1, ch2, k1, k2 = model.conv1.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 3, 7, 7])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.conv1.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_linear = torch.nn.Linear(64, 32)\n",
    "reshaped_weight = model.conv1.weight.data.view(64, -1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_pruning = check_linear(reshaped_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 64])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_linear.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight: 0\n",
      "conv1.bias: 0\n",
      "layer1.0.conv1.weight: 1\n",
      "layer1.0.conv2.weight: 0\n",
      "layer1.0.conv2.bias: 0\n",
      "layer1.1.conv1.weight: 1\n",
      "layer1.1.conv2.weight: 0\n",
      "layer1.1.conv2.bias: 0\n",
      "layer2.0.conv1.weight: 1\n",
      "layer2.0.downsample.0.weight: 1\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torchvision\n",
    "\n",
    "root = os.path.expanduser(\"~\") + \"/datasets/\"\n",
    "batch_size = 64\n",
    "augmentation = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.RandomHorizontalFlip(),\n",
    "        torchvision.transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocess = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    root=root, train=True, download=True, transform=augmentation\n",
    ")\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "\n",
    "val_dataset = torchvision.datasets.CIFAR10(\n",
    "    root=root, train=False, download=True, transform=preprocess\n",
    ")\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size, shuffle=False)\n",
    "loaders = {\"train\": train_dataloader, \"valid\": val_dataloader}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch_integral\n",
    "\n",
    "continuous_dims = torch_integral.standard_continuous_dims(model)\n",
    "# 'fc.weight': [0, 1],\n",
    "# 'fc.bias': [0]\n",
    "# 'conv1.weight': [0, 1]\n",
    "continuous_dims['conv1.weight'] = [0]\n",
    "continuous_dims['fc.weight'] = [1]\n",
    "continuous_dims.pop('fc.bias', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rearranging of group 0\n",
      "variation before permutation: 87.49198150634766\n",
      "variation after permutation: 50.8090705871582\n",
      "Rearranging of group 1\n",
      "variation before permutation: 32.05695343017578\n",
      "variation after permutation: 8.883305549621582\n",
      "Rearranging of group 2\n",
      "variation before permutation: 41.3999137878418\n",
      "variation after permutation: 10.537375450134277\n",
      "Rearranging of group 3\n",
      "variation before permutation: 34.390079498291016\n",
      "variation after permutation: 10.334465980529785\n",
      "Rearranging of group 4\n",
      "variation before permutation: 141.0234375\n",
      "variation after permutation: 72.79061126708984\n",
      "Rearranging of group 5\n",
      "variation before permutation: 42.25988006591797\n",
      "variation after permutation: 12.976730346679688\n",
      "Rearranging of group 6\n",
      "variation before permutation: 73.67745208740234\n",
      "variation after permutation: 16.51487159729004\n",
      "Rearranging of group 7\n",
      "variation before permutation: 170.888916015625\n",
      "variation after permutation: 84.26046752929688\n",
      "Rearranging of group 8\n",
      "variation before permutation: 86.69864654541016\n",
      "variation after permutation: 18.89717674255371\n",
      "Rearranging of group 9\n",
      "variation before permutation: 137.10707092285156\n",
      "variation after permutation: 39.414756774902344\n",
      "Rearranging of group 10\n",
      "variation before permutation: 595.0445556640625\n",
      "variation after permutation: 315.09161376953125\n",
      "Rearranging of group 11\n",
      "variation before permutation: 172.2285919189453\n",
      "variation after permutation: 131.2566680908203\n",
      "conv1.weight\n",
      "loss before optimization:  4.416936736202854e-15\n",
      "conv1.bias\n",
      "loss before optimization:  1.3330913521507337e-13\n",
      "layer1.0.conv1.weight\n",
      "loss before optimization:  7.696259096332241e-15\n",
      "layer1.0.conv2.weight\n",
      "loss before optimization:  3.9128690386544193e-14\n",
      "layer1.0.conv2.bias\n",
      "loss before optimization:  1.882927949843627e-13\n",
      "layer1.1.conv1.weight\n",
      "loss before optimization:  6.40751794884261e-15\n",
      "layer1.1.conv2.weight\n",
      "loss before optimization:  8.048685280542464e-14\n",
      "layer1.1.conv2.bias\n",
      "loss before optimization:  2.756553123782479e-13\n",
      "layer2.0.conv1.weight\n",
      "loss before optimization:  3.119732670778968e-15\n",
      "layer2.0.downsample.0.weight\n",
      "loss before optimization:  3.66038782942911e-14\n",
      "layer1.0.conv1.bias\n",
      "loss before optimization:  2.551355394314614e-13\n",
      "layer1.1.conv1.bias\n",
      "loss before optimization:  6.922397767852861e-14\n",
      "layer2.0.conv1.bias\n",
      "loss before optimization:  5.196506473823664e-15\n",
      "layer2.0.conv2.weight\n",
      "loss before optimization:  3.1566017703356375e-14\n",
      "layer2.0.downsample.0.bias\n",
      "loss before optimization:  3.5501606537348407e-13\n",
      "layer2.0.conv2.bias\n",
      "loss before optimization:  4.312876282237116e-13\n",
      "layer2.1.conv1.weight\n",
      "loss before optimization:  6.719973156491671e-15\n",
      "layer2.1.conv2.weight\n",
      "loss before optimization:  3.7115181262571684e-14\n",
      "layer2.1.conv2.bias\n",
      "loss before optimization:  5.869043315578915e-13\n",
      "layer3.0.conv1.weight\n",
      "loss before optimization:  3.22029898678234e-14\n",
      "layer3.0.downsample.0.weight\n",
      "loss before optimization:  5.251777406511081e-14\n",
      "layer2.1.conv1.bias\n",
      "loss before optimization:  8.376080455315196e-14\n",
      "layer3.0.conv1.bias\n",
      "loss before optimization:  2.0080126255338226e-13\n",
      "layer3.0.conv2.weight\n",
      "loss before optimization:  1.2503911209389706e-13\n",
      "layer3.0.downsample.0.bias\n",
      "loss before optimization:  2.1101649344979628e-13\n",
      "layer3.0.conv2.bias\n",
      "loss before optimization:  3.7116192281097526e-13\n",
      "layer3.1.conv1.weight\n",
      "loss before optimization:  5.186485396431288e-14\n",
      "layer3.1.conv2.weight\n",
      "loss before optimization:  3.0788392668679165e-13\n",
      "layer3.1.conv2.bias\n",
      "loss before optimization:  6.517718222770474e-13\n",
      "layer4.0.conv1.weight\n",
      "loss before optimization:  4.0678543839585066e-14\n",
      "layer4.0.downsample.0.weight\n",
      "loss before optimization:  6.577600877161194e-13\n",
      "layer3.1.conv1.bias\n",
      "loss before optimization:  6.706653732802687e-14\n",
      "layer4.0.conv1.bias\n",
      "loss before optimization:  1.488906213760726e-14\n",
      "layer4.0.conv2.weight\n",
      "loss before optimization:  5.046729635756197e-13\n",
      "layer4.0.downsample.0.bias\n",
      "loss before optimization:  6.142358214937527e-13\n",
      "layer4.0.conv2.bias\n",
      "loss before optimization:  1.723250231747131e-12\n",
      "layer4.1.conv1.weight\n",
      "loss before optimization:  4.135790830899627e-14\n",
      "layer4.1.conv2.weight\n",
      "loss before optimization:  9.185877752893035e-12\n",
      "layer4.1.conv2.bias\n",
      "loss before optimization:  9.813621269783024e-13\n",
      "fc.weight\n",
      "loss before optimization:  4.742620684193566e-13\n",
      "layer4.1.conv1.bias\n",
      "loss before optimization:  1.6275902405883148e-14\n"
     ]
    }
   ],
   "source": [
    "integral_model = torch_integral.IntegralWrapper(\n",
    "    init_from_discrete=True, verbose=True\n",
    ")(model, input_example, continuous_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpylogger\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catalyst import dl\n",
    "cross_entropy = torch.nn.CrossEntropyLoss()\n",
    "log_dir = \"./logs/cifar\"\n",
    "runner = dl.SupervisedRunner(\n",
    "    input_key=\"features\", output_key=\"logits\", target_key=\"targets\", loss_key=\"loss\"\n",
    ")\n",
    "callbacks = [\n",
    "    dl.AccuracyCallback(\n",
    "        input_key=\"logits\", target_key=\"targets\", topk=(1,), num_classes=10\n",
    "    ),\n",
    "    dl.SchedulerCallback(mode=\"batch\", loader_key=\"train\", metric_key=\"loss\"),\n",
    "]\n",
    "loggers = []\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compression:  0.4701610103869056\n"
     ]
    }
   ],
   "source": [
    "sparsity = 0.5\n",
    "for group in integral_model.groups:\n",
    "    if \"operator\" not in group.operations:\n",
    "        initial_size = group.size\n",
    "        new_size = int(initial_size * sparsity)\n",
    "        group.reset_grid(\n",
    "            torch_integral.TrainableGrid1D(new_size)\n",
    "        )\n",
    "\n",
    "print(\"compression: \", integral_model.eval().calculate_compression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e0638dfa33844439024381a3d5d7d0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1/10 * Epoch (train):   0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard Interrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m epoch_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_dataloader)\n\u001b[1;32m      4\u001b[0m sched \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mMultiStepLR(\n\u001b[1;32m      5\u001b[0m     opt, [epoch_len \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, epoch_len \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m5\u001b[39m, epoch_len \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m6\u001b[39m, epoch_len \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m8\u001b[39m], gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.33\u001b[39m\n\u001b[1;32m      6\u001b[0m )\n\u001b[0;32m----> 7\u001b[0m \u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mintegral_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msched\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloggers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloggers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogdir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mloss\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/hse_diploma/lib/python3.11/site-packages/catalyst/runners/runner.py:377\u001b[0m, in \u001b[0;36mRunner.train\u001b[0;34m(self, loaders, model, engine, criterion, optimizer, scheduler, callbacks, loggers, seed, hparams, num_epochs, logdir, resume, valid_loader, valid_metric, minimize_valid_metric, verbose, timeit, check, overfit, profile, load_best_on_end, cpu, fp16, ddp)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_best_on_end \u001b[38;5;241m=\u001b[39m load_best_on_end\n\u001b[1;32m    376\u001b[0m \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[0;32m--> 377\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/hse_diploma/lib/python3.11/site-packages/catalyst/core/runner.py:422\u001b[0m, in \u001b[0;36mIRunner.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mException\u001b[39;00m, \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexception \u001b[38;5;241m=\u001b[39m ex\n\u001b[0;32m--> 422\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_event\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mon_exception\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/hse_diploma/lib/python3.11/site-packages/catalyst/core/runner.py:365\u001b[0m, in \u001b[0;36mIRunner._run_event\u001b[0;34m(self, event)\u001b[0m\n\u001b[1;32m    363\u001b[0m     \u001b[38;5;28mgetattr\u001b[39m(callback, event)(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    364\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_str_intersections(event, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_end\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_exception\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[0;32m--> 365\u001b[0m     \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/hse_diploma/lib/python3.11/site-packages/catalyst/core/runner.py:357\u001b[0m, in \u001b[0;36mIRunner.on_exception\u001b[0;34m(self, runner)\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_exception\u001b[39m(\u001b[38;5;28mself\u001b[39m, runner: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIRunner\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    356\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Event handler.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 357\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexception\n",
      "File \u001b[0;32m~/miniconda3/envs/hse_diploma/lib/python3.11/site-packages/catalyst/core/runner.py:419\u001b[0m, in \u001b[0;36mIRunner.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Runs the experiment.\u001b[39;00m\n\u001b[1;32m    414\u001b[0m \n\u001b[1;32m    415\u001b[0m \u001b[38;5;124;03mReturns:\u001b[39;00m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;124;03m    self, `IRunner` instance after the experiment\u001b[39;00m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 419\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mException\u001b[39;00m, \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexception \u001b[38;5;241m=\u001b[39m ex\n",
      "File \u001b[0;32m~/miniconda3/envs/hse_diploma/lib/python3.11/site-packages/catalyst/core/runner.py:410\u001b[0m, in \u001b[0;36mIRunner._run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    409\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_engine()\n\u001b[0;32m--> 410\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspawn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_local\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/hse_diploma/lib/python3.11/site-packages/catalyst/core/engine.py:59\u001b[0m, in \u001b[0;36mEngine.spawn\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mspawn\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn: Callable, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     43\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Spawns processes with specified ``fn`` and ``args``/``kwargs``.\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \n\u001b[1;32m     45\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;124;03m        wrapped function (if needed).\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/hse_diploma/lib/python3.11/site-packages/catalyst/core/runner.py:405\u001b[0m, in \u001b[0;36mIRunner._run_local\u001b[0;34m(self, local_rank, world_size)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_local_rank, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_world_size \u001b[38;5;241m=\u001b[39m local_rank, world_size\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_event(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_experiment_start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 405\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_event(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_experiment_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/hse_diploma/lib/python3.11/site-packages/catalyst/core/runner.py:399\u001b[0m, in \u001b[0;36mIRunner._run_experiment\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_event(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_epoch_start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 399\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_event(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_epoch_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/hse_diploma/lib/python3.11/site-packages/catalyst/core/runner.py:391\u001b[0m, in \u001b[0;36mIRunner._run_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloaders\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_event(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_loader_start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 391\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_event(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_loader_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/hse_diploma/lib/python3.11/site-packages/catalyst/core/runner.py:386\u001b[0m, in \u001b[0;36mIRunner._run_loader\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_event(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_batch_start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_batch(batch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch)\n\u001b[0;32m--> 386\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_event\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mon_batch_end\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/hse_diploma/lib/python3.11/site-packages/catalyst/core/runner.py:363\u001b[0m, in \u001b[0;36mIRunner._run_event\u001b[0;34m(self, event)\u001b[0m\n\u001b[1;32m    361\u001b[0m     \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, event)(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[0;32m--> 363\u001b[0m     \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_str_intersections(event, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_end\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_exception\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[1;32m    365\u001b[0m     \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, event)(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/hse_diploma/lib/python3.11/site-packages/catalyst/callbacks/backward.py:51\u001b[0m, in \u001b[0;36mBackwardCallback.on_batch_end\u001b[0;34m(self, runner)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mis_train_loader:\n\u001b[1;32m     50\u001b[0m     loss \u001b[38;5;241m=\u001b[39m runner\u001b[38;5;241m.\u001b[39mbatch_metrics[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetric_key]\n\u001b[0;32m---> 51\u001b[0m     \u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad_clip_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m         runner\u001b[38;5;241m.\u001b[39mengine\u001b[38;5;241m.\u001b[39munscale_gradients()\n",
      "File \u001b[0;32m~/miniconda3/envs/hse_diploma/lib/python3.11/site-packages/accelerate/accelerator.py:1994\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   1991\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED:\n\u001b[1;32m   1992\u001b[0m     \u001b[38;5;66;03m# deepspeed handles loss scaling by gradient_accumulation_steps in its `backward`\u001b[39;00m\n\u001b[1;32m   1993\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n\u001b[0;32m-> 1994\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistributed_type\u001b[49m \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED:\n\u001b[1;32m   1995\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeepspeed_engine_wrapped\u001b[38;5;241m.\u001b[39mbackward(loss, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1996\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mMEGATRON_LM:\n",
      "File \u001b[0;32m~/miniconda3/envs/hse_diploma/lib/python3.11/site-packages/accelerate/accelerator.py:507\u001b[0m, in \u001b[0;36mAccelerator.distributed_type\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;124;03m    Whether the Accelerator is configured for distributed training\u001b[39;00m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    505\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39muse_distributed\n\u001b[0;32m--> 507\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdistributed_type\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mdistributed_type\n\u001b[1;32m    511\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnum_processes\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "with torch_integral.grid_tuning(integral_model, False, True):\n",
    "    opt = torch.optim.Adam(integral_model.parameters(), lr=1e-3, weight_decay=0)\n",
    "    epoch_len = len(train_dataloader)\n",
    "    sched = torch.optim.lr_scheduler.MultiStepLR(\n",
    "        opt, [epoch_len * 2, epoch_len * 5, epoch_len * 6, epoch_len * 8], gamma=0.33\n",
    "    )\n",
    "    runner.train(\n",
    "        model=integral_model,\n",
    "        criterion=cross_entropy,\n",
    "        optimizer=opt,\n",
    "        scheduler=sched,\n",
    "        loaders=loaders,\n",
    "        num_epochs=epochs,\n",
    "        callbacks=callbacks,\n",
    "        loggers=loggers,\n",
    "        logdir=log_dir,\n",
    "        valid_loader=\"valid\",\n",
    "        valid_metric=\"loss\",\n",
    "        verbose=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for group in groups:\n",
    "#     for param in group.params:\n",
    "#         name = param['name'].replace('.weight', '').replace('.bias', '')\n",
    "#         submodule = model.get_submodule(name)\n",
    "#         if 'weight' in param['name']:\n",
    "#             weight = submodule.weight.data\n",
    "#         elif 'bias' in param['name']:\n",
    "#             weight = submodule.bias.data\n",
    "#         print(name, param['dim'], weight.shape == param['value'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hasattr(torch.nn, target_layer.__class__.__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for gr in groups:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
       "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
       "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
       "        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
       "        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
       "        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
       "        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
       "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
       "       104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
       "       117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
       "       130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n",
       "       143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
       "       156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n",
       "       169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
       "       182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194,\n",
       "       195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207,\n",
       "       208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220,\n",
       "       221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
       "       234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246,\n",
       "       247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259,\n",
       "       260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272,\n",
       "       273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285,\n",
       "       286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298,\n",
       "       299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311,\n",
       "       312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324,\n",
       "       325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337,\n",
       "       338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350,\n",
       "       351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
       "       364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376,\n",
       "       377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389,\n",
       "       390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402,\n",
       "       403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415,\n",
       "       416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428,\n",
       "       429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441,\n",
       "       442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454,\n",
       "       455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467,\n",
       "       468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480,\n",
       "       481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493,\n",
       "       494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506,\n",
       "       507, 508, 509, 510, 511])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.arange(0, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(prune_out_channels on layer4.1.conv2 (Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))) => prune_out_channels on layer4.1.conv2 (Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))), [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511]),\n",
       " (prune_out_channels on layer4.1.conv2 (Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))) => prune_out_channels on _ElementWiseOp_4(AddBackward0), [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511]),\n",
       " (prune_out_channels on _ElementWiseOp_4(AddBackward0) => prune_out_channels on _ElementWiseOp_5(ReluBackward0), [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511]),\n",
       " (prune_out_channels on _ElementWiseOp_4(AddBackward0) => prune_out_channels on _ElementWiseOp_3(ReluBackward0), [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511]),\n",
       " (prune_out_channels on _ElementWiseOp_3(ReluBackward0) => prune_out_channels on _ElementWiseOp_2(MeanBackward1), [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511]),\n",
       " (prune_out_channels on _ElementWiseOp_2(MeanBackward1) => prune_out_channels on _Reshape_0(), [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511]),\n",
       " (prune_out_channels on _Reshape_0() => prune_in_channels on fc (Linear(in_features=512, out_features=1000, bias=True)), [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511]),\n",
       " (prune_in_channels on fc (Linear(in_features=512, out_features=1000, bias=True)) => prune_out_channels on _ElementWiseOp_1(TBackward0), [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511]),\n",
       " (prune_out_channels on _ElementWiseOp_5(ReluBackward0) => prune_out_channels on _ElementWiseOp_6(AddBackward0), [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511]),\n",
       " (prune_out_channels on _ElementWiseOp_5(ReluBackward0) => prune_in_channels on layer4.1.conv1 (Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))), [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511]),\n",
       " (prune_out_channels on _ElementWiseOp_6(AddBackward0) => prune_out_channels on layer4.0.conv2 (Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))), [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511]),\n",
       " (prune_out_channels on _ElementWiseOp_6(AddBackward0) => prune_out_channels on layer4.0.downsample.0 (Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2))), [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511])]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr._group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer4.1.conv2 prune_out_channels\n",
      "layer4.1.conv2 prune_out_channels\n",
      "layer4.1.conv2 prune_out_channels\n",
      "fc prune_in_channels\n",
      "fc prune_out_channels\n",
      "layer4.1.conv1 prune_in_channels\n",
      "layer4.0.conv2 prune_out_channels\n",
      "layer4.0.downsample.0 prune_out_channels\n",
      "--------------------------------------------------------------------------------\n",
      "layer3.1.conv2 prune_out_channels\n",
      "layer3.1.conv2 prune_out_channels\n",
      "layer3.1.conv2 prune_out_channels\n",
      "layer4.0.downsample.0 prune_in_channels\n",
      "layer4.0.conv1 prune_in_channels\n",
      "layer3.1.conv1 prune_in_channels\n",
      "layer3.0.conv2 prune_out_channels\n",
      "layer3.0.downsample.0 prune_out_channels\n",
      "--------------------------------------------------------------------------------\n",
      "layer2.1.conv2 prune_out_channels\n",
      "layer2.1.conv2 prune_out_channels\n",
      "layer2.1.conv2 prune_out_channels\n",
      "layer3.0.downsample.0 prune_in_channels\n",
      "layer3.0.conv1 prune_in_channels\n",
      "layer2.1.conv1 prune_in_channels\n",
      "layer2.0.conv2 prune_out_channels\n",
      "layer2.0.downsample.0 prune_out_channels\n",
      "--------------------------------------------------------------------------------\n",
      "layer1.1.conv2 prune_out_channels\n",
      "layer1.1.conv2 prune_out_channels\n",
      "layer1.1.conv2 prune_out_channels\n",
      "layer2.0.downsample.0 prune_in_channels\n",
      "layer2.0.conv1 prune_in_channels\n",
      "layer1.1.conv1 prune_in_channels\n",
      "layer1.0.conv2 prune_out_channels\n",
      "layer1.0.conv1 prune_in_channels\n",
      "conv1 prune_out_channels\n",
      "--------------------------------------------------------------------------------\n",
      "layer1.0.conv1 prune_out_channels\n",
      "layer1.0.conv1 prune_out_channels\n",
      "layer1.0.conv1 prune_out_channels\n",
      "layer1.0.conv2 prune_in_channels\n",
      "--------------------------------------------------------------------------------\n",
      "layer1.1.conv1 prune_out_channels\n",
      "layer1.1.conv1 prune_out_channels\n",
      "layer1.1.conv1 prune_out_channels\n",
      "layer1.1.conv2 prune_in_channels\n",
      "--------------------------------------------------------------------------------\n",
      "layer2.0.conv1 prune_out_channels\n",
      "layer2.0.conv1 prune_out_channels\n",
      "layer2.0.conv1 prune_out_channels\n",
      "layer2.0.conv2 prune_in_channels\n",
      "--------------------------------------------------------------------------------\n",
      "layer2.1.conv1 prune_out_channels\n",
      "layer2.1.conv1 prune_out_channels\n",
      "layer2.1.conv1 prune_out_channels\n",
      "layer2.1.conv2 prune_in_channels\n",
      "--------------------------------------------------------------------------------\n",
      "layer3.0.conv1 prune_out_channels\n",
      "layer3.0.conv1 prune_out_channels\n",
      "layer3.0.conv1 prune_out_channels\n",
      "layer3.0.conv2 prune_in_channels\n",
      "--------------------------------------------------------------------------------\n",
      "layer3.1.conv1 prune_out_channels\n",
      "layer3.1.conv1 prune_out_channels\n",
      "layer3.1.conv1 prune_out_channels\n",
      "layer3.1.conv2 prune_in_channels\n",
      "--------------------------------------------------------------------------------\n",
      "layer4.0.conv1 prune_out_channels\n",
      "layer4.0.conv1 prune_out_channels\n",
      "layer4.0.conv1 prune_out_channels\n",
      "layer4.0.conv2 prune_in_channels\n",
      "--------------------------------------------------------------------------------\n",
      "layer4.1.conv1 prune_out_channels\n",
      "layer4.1.conv1 prune_out_channels\n",
      "layer4.1.conv1 prune_out_channels\n",
      "layer4.1.conv2 prune_in_channels\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for group in :\n",
    "    # print(group)\n",
    "    for i, (dep, idxs) in enumerate(group):\n",
    "        trigger = dep.trigger\n",
    "        handler = dep.handler\n",
    "        source_layer = dep.source.module\n",
    "        target_layer = dep.target.module\n",
    "        if hasattr(torch.nn, target_layer.__class__.__name__):\n",
    "            print(dep.target.name.split()[0], handler.__name__)\n",
    "        \n",
    "        if hasattr(torch.nn, source_layer.__class__.__name__):\n",
    "            print(dep.source.name.split()[0], handler.__name__)\n",
    "            \n",
    "    print('-' * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prune_out_channels on layer4.0.downsample.0 (Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)) => prune_out_channels on layer4.0.downsample.0 (Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False))\n",
      "For Dep:  prune_out_channels on layer4.0.downsample.0 (Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)) => prune_out_channels on layer4.0.downsample.0 (Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False))\n",
      "Handler:  prune_out_channels\n",
      " > Source Layer:  Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      " > Target Layer:  Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "\n",
      "prune_out_channels on layer4.0.downsample.0 (Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)) => prune_out_channels on layer4.0.downsample.1 (BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
      "For Dep:  prune_out_channels on layer4.0.downsample.0 (Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)) => prune_out_channels on layer4.0.downsample.1 (BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
      "Handler:  prune_out_channels\n",
      " > Source Layer:  Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      " > Target Layer:  BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "\n",
      "prune_out_channels on layer4.0.downsample.1 (BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)) => prune_out_channels on _ElementWiseOp_6(AddBackward0)\n",
      "For Dep:  prune_out_channels on layer4.0.downsample.1 (BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)) => prune_out_channels on _ElementWiseOp_6(AddBackward0)\n",
      "Handler:  prune_out_channels\n",
      " > Source Layer:  BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      " > Target Layer:  _ElementWiseOp_6(AddBackward0)\n",
      "\n",
      "prune_out_channels on _ElementWiseOp_6(AddBackward0) => prune_out_channels on layer4.0.bn2 (BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
      "For Dep:  prune_out_channels on _ElementWiseOp_6(AddBackward0) => prune_out_channels on layer4.0.bn2 (BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
      "Handler:  prune_out_channels\n",
      " > Source Layer:  _ElementWiseOp_6(AddBackward0)\n",
      " > Target Layer:  BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "\n",
      "prune_out_channels on _ElementWiseOp_6(AddBackward0) => prune_out_channels on _ElementWiseOp_5(ReluBackward0)\n",
      "For Dep:  prune_out_channels on _ElementWiseOp_6(AddBackward0) => prune_out_channels on _ElementWiseOp_5(ReluBackward0)\n",
      "Handler:  prune_out_channels\n",
      " > Source Layer:  _ElementWiseOp_6(AddBackward0)\n",
      " > Target Layer:  _ElementWiseOp_5(ReluBackward0)\n",
      "\n",
      "prune_out_channels on _ElementWiseOp_5(ReluBackward0) => prune_out_channels on _ElementWiseOp_4(AddBackward0)\n",
      "For Dep:  prune_out_channels on _ElementWiseOp_5(ReluBackward0) => prune_out_channels on _ElementWiseOp_4(AddBackward0)\n",
      "Handler:  prune_out_channels\n",
      " > Source Layer:  _ElementWiseOp_5(ReluBackward0)\n",
      " > Target Layer:  _ElementWiseOp_4(AddBackward0)\n",
      "\n",
      "prune_out_channels on _ElementWiseOp_5(ReluBackward0) => prune_in_channels on layer4.1.conv1 (Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False))\n",
      "For Dep:  prune_out_channels on _ElementWiseOp_5(ReluBackward0) => prune_in_channels on layer4.1.conv1 (Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False))\n",
      "Handler:  prune_in_channels\n",
      " > Source Layer:  _ElementWiseOp_5(ReluBackward0)\n",
      " > Target Layer:  Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "\n",
      "prune_out_channels on _ElementWiseOp_4(AddBackward0) => prune_out_channels on layer4.1.bn2 (BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
      "For Dep:  prune_out_channels on _ElementWiseOp_4(AddBackward0) => prune_out_channels on layer4.1.bn2 (BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
      "Handler:  prune_out_channels\n",
      " > Source Layer:  _ElementWiseOp_4(AddBackward0)\n",
      " > Target Layer:  BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "\n",
      "prune_out_channels on _ElementWiseOp_4(AddBackward0) => prune_out_channels on _ElementWiseOp_3(ReluBackward0)\n",
      "For Dep:  prune_out_channels on _ElementWiseOp_4(AddBackward0) => prune_out_channels on _ElementWiseOp_3(ReluBackward0)\n",
      "Handler:  prune_out_channels\n",
      " > Source Layer:  _ElementWiseOp_4(AddBackward0)\n",
      " > Target Layer:  _ElementWiseOp_3(ReluBackward0)\n",
      "\n",
      "prune_out_channels on _ElementWiseOp_3(ReluBackward0) => prune_out_channels on _ElementWiseOp_2(MeanBackward1)\n",
      "For Dep:  prune_out_channels on _ElementWiseOp_3(ReluBackward0) => prune_out_channels on _ElementWiseOp_2(MeanBackward1)\n",
      "Handler:  prune_out_channels\n",
      " > Source Layer:  _ElementWiseOp_3(ReluBackward0)\n",
      " > Target Layer:  _ElementWiseOp_2(MeanBackward1)\n",
      "\n",
      "prune_out_channels on _ElementWiseOp_2(MeanBackward1) => prune_out_channels on _Reshape_0()\n",
      "For Dep:  prune_out_channels on _ElementWiseOp_2(MeanBackward1) => prune_out_channels on _Reshape_0()\n",
      "Handler:  prune_out_channels\n",
      " > Source Layer:  _ElementWiseOp_2(MeanBackward1)\n",
      " > Target Layer:  _Reshape_0()\n",
      "\n",
      "prune_out_channels on _Reshape_0() => prune_in_channels on fc (Linear(in_features=512, out_features=1000, bias=True))\n",
      "For Dep:  prune_out_channels on _Reshape_0() => prune_in_channels on fc (Linear(in_features=512, out_features=1000, bias=True))\n",
      "Handler:  prune_in_channels\n",
      " > Source Layer:  _Reshape_0()\n",
      " > Target Layer:  Linear(in_features=512, out_features=1000, bias=True)\n",
      "\n",
      "prune_in_channels on fc (Linear(in_features=512, out_features=1000, bias=True)) => prune_out_channels on _ElementWiseOp_1(TBackward0)\n",
      "For Dep:  prune_in_channels on fc (Linear(in_features=512, out_features=1000, bias=True)) => prune_out_channels on _ElementWiseOp_1(TBackward0)\n",
      "Handler:  prune_out_channels\n",
      " > Source Layer:  Linear(in_features=512, out_features=1000, bias=True)\n",
      " > Target Layer:  _ElementWiseOp_1(TBackward0)\n",
      "\n",
      "prune_out_channels on layer4.1.bn2 (BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)) => prune_out_channels on layer4.1.conv2 (Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False))\n",
      "For Dep:  prune_out_channels on layer4.1.bn2 (BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)) => prune_out_channels on layer4.1.conv2 (Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False))\n",
      "Handler:  prune_out_channels\n",
      " > Source Layer:  BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      " > Target Layer:  Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "\n",
      "prune_out_channels on layer4.0.bn2 (BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)) => prune_out_channels on layer4.0.conv2 (Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False))\n",
      "For Dep:  prune_out_channels on layer4.0.bn2 (BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)) => prune_out_channels on layer4.0.conv2 (Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False))\n",
      "Handler:  prune_out_channels\n",
      " > Source Layer:  BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      " > Target Layer:  Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, (dep, idxs) in enumerate(group):\n",
    "    trigger = dep.trigger\n",
    "    handler = dep.handler\n",
    "    source_layer = dep.source.module\n",
    "    target_layer = dep.target.module\n",
    "    print(dep)\n",
    "    print(\"For Dep: \", dep)\n",
    "    print(\"Handler: \", handler.__name__)\n",
    "    print(\" > Source Layer: \", source_layer)\n",
    "    print(\" > Target Layer: \", target_layer)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.fx.Interpret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight: 0\n",
      "layer1.0.conv1.weight: 1\n",
      "layer1.0.conv2.weight: 0\n",
      "layer1.1.conv1.weight: 1\n",
      "layer1.1.conv2.weight: 0\n",
      "layer2.0.conv1.weight: 1\n",
      "layer2.0.downsample.0.weight: 1\n",
      "\n",
      "7\n",
      "conv1.weight: 2\n",
      "\n",
      "1\n",
      "conv1.weight: 3\n",
      "\n",
      "1\n",
      "bn1.weight: 0\n",
      "\n",
      "1\n",
      "bn1.bias: 0\n",
      "\n",
      "1\n",
      "layer1.0.conv1.weight: 0\n",
      "layer1.0.conv2.weight: 1\n",
      "\n",
      "2\n",
      "layer1.0.conv1.weight: 2\n",
      "\n",
      "1\n",
      "layer1.0.conv1.weight: 3\n",
      "\n",
      "1\n",
      "layer1.0.bn1.weight: 0\n",
      "\n",
      "1\n",
      "layer1.0.bn1.bias: 0\n",
      "\n",
      "1\n",
      "layer1.0.conv2.weight: 2\n",
      "\n",
      "1\n",
      "layer1.0.conv2.weight: 3\n",
      "\n",
      "1\n",
      "layer1.0.bn2.weight: 0\n",
      "\n",
      "1\n",
      "layer1.0.bn2.bias: 0\n",
      "\n",
      "1\n",
      "layer1.1.conv1.weight: 0\n",
      "layer1.1.conv2.weight: 1\n",
      "\n",
      "2\n",
      "layer1.1.conv1.weight: 2\n",
      "\n",
      "1\n",
      "layer1.1.conv1.weight: 3\n",
      "\n",
      "1\n",
      "layer1.1.bn1.weight: 0\n",
      "\n",
      "1\n",
      "layer1.1.bn1.bias: 0\n",
      "\n",
      "1\n",
      "layer1.1.conv2.weight: 2\n",
      "\n",
      "1\n",
      "layer1.1.conv2.weight: 3\n",
      "\n",
      "1\n",
      "layer1.1.bn2.weight: 0\n",
      "\n",
      "1\n",
      "layer1.1.bn2.bias: 0\n",
      "\n",
      "1\n",
      "layer2.0.conv1.weight: 0\n",
      "layer2.0.conv2.weight: 1\n",
      "\n",
      "2\n",
      "layer2.0.conv1.weight: 2\n",
      "\n",
      "1\n",
      "layer2.0.conv1.weight: 3\n",
      "\n",
      "1\n",
      "layer2.0.bn1.weight: 0\n",
      "\n",
      "1\n",
      "layer2.0.bn1.bias: 0\n",
      "\n",
      "1\n",
      "layer2.0.conv2.weight: 2\n",
      "\n",
      "1\n",
      "layer2.0.conv2.weight: 3\n",
      "\n",
      "1\n",
      "layer2.0.bn2.weight: 0\n",
      "\n",
      "1\n",
      "layer2.0.bn2.bias: 0\n",
      "\n",
      "1\n",
      "layer2.0.downsample.0.weight: 0\n",
      "layer2.0.conv2.weight: 0\n",
      "layer2.1.conv1.weight: 1\n",
      "layer2.1.conv2.weight: 0\n",
      "layer3.0.conv1.weight: 1\n",
      "layer3.0.downsample.0.weight: 1\n",
      "\n",
      "6\n",
      "layer2.0.downsample.0.weight: 2\n",
      "\n",
      "1\n",
      "layer2.0.downsample.0.weight: 3\n",
      "\n",
      "1\n",
      "layer2.0.downsample.1.weight: 0\n",
      "\n",
      "1\n",
      "layer2.0.downsample.1.bias: 0\n",
      "\n",
      "1\n",
      "layer2.1.conv1.weight: 0\n",
      "layer2.1.conv2.weight: 1\n",
      "\n",
      "2\n",
      "layer2.1.conv1.weight: 2\n",
      "\n",
      "1\n",
      "layer2.1.conv1.weight: 3\n",
      "\n",
      "1\n",
      "layer2.1.bn1.weight: 0\n",
      "\n",
      "1\n",
      "layer2.1.bn1.bias: 0\n",
      "\n",
      "1\n",
      "layer2.1.conv2.weight: 2\n",
      "\n",
      "1\n",
      "layer2.1.conv2.weight: 3\n",
      "\n",
      "1\n",
      "layer2.1.bn2.weight: 0\n",
      "\n",
      "1\n",
      "layer2.1.bn2.bias: 0\n",
      "\n",
      "1\n",
      "layer3.0.conv1.weight: 0\n",
      "layer3.0.conv2.weight: 1\n",
      "\n",
      "2\n",
      "layer3.0.conv1.weight: 2\n",
      "\n",
      "1\n",
      "layer3.0.conv1.weight: 3\n",
      "\n",
      "1\n",
      "layer3.0.bn1.weight: 0\n",
      "\n",
      "1\n",
      "layer3.0.bn1.bias: 0\n",
      "\n",
      "1\n",
      "layer3.0.conv2.weight: 2\n",
      "\n",
      "1\n",
      "layer3.0.conv2.weight: 3\n",
      "\n",
      "1\n",
      "layer3.0.bn2.weight: 0\n",
      "\n",
      "1\n",
      "layer3.0.bn2.bias: 0\n",
      "\n",
      "1\n",
      "layer3.0.downsample.0.weight: 0\n",
      "layer3.0.conv2.weight: 0\n",
      "layer3.1.conv1.weight: 1\n",
      "layer3.1.conv2.weight: 0\n",
      "layer4.0.conv1.weight: 1\n",
      "layer4.0.downsample.0.weight: 1\n",
      "\n",
      "6\n",
      "layer3.0.downsample.0.weight: 2\n",
      "\n",
      "1\n",
      "layer3.0.downsample.0.weight: 3\n",
      "\n",
      "1\n",
      "layer3.0.downsample.1.weight: 0\n",
      "\n",
      "1\n",
      "layer3.0.downsample.1.bias: 0\n",
      "\n",
      "1\n",
      "layer3.1.conv1.weight: 0\n",
      "layer3.1.conv2.weight: 1\n",
      "\n",
      "2\n",
      "layer3.1.conv1.weight: 2\n",
      "\n",
      "1\n",
      "layer3.1.conv1.weight: 3\n",
      "\n",
      "1\n",
      "layer3.1.bn1.weight: 0\n",
      "\n",
      "1\n",
      "layer3.1.bn1.bias: 0\n",
      "\n",
      "1\n",
      "layer3.1.conv2.weight: 2\n",
      "\n",
      "1\n",
      "layer3.1.conv2.weight: 3\n",
      "\n",
      "1\n",
      "layer3.1.bn2.weight: 0\n",
      "\n",
      "1\n",
      "layer3.1.bn2.bias: 0\n",
      "\n",
      "1\n",
      "layer4.0.conv1.weight: 0\n",
      "layer4.0.conv2.weight: 1\n",
      "\n",
      "2\n",
      "layer4.0.conv1.weight: 2\n",
      "\n",
      "1\n",
      "layer4.0.conv1.weight: 3\n",
      "\n",
      "1\n",
      "layer4.0.bn1.weight: 0\n",
      "\n",
      "1\n",
      "layer4.0.bn1.bias: 0\n",
      "\n",
      "1\n",
      "layer4.0.conv2.weight: 2\n",
      "\n",
      "1\n",
      "layer4.0.conv2.weight: 3\n",
      "\n",
      "1\n",
      "layer4.0.bn2.weight: 0\n",
      "\n",
      "1\n",
      "layer4.0.bn2.bias: 0\n",
      "\n",
      "1\n",
      "layer4.0.downsample.0.weight: 0\n",
      "layer4.0.conv2.weight: 0\n",
      "layer4.1.conv1.weight: 1\n",
      "layer4.1.conv2.weight: 0\n",
      "fc.weight: 1\n",
      "\n",
      "5\n",
      "layer4.0.downsample.0.weight: 2\n",
      "\n",
      "1\n",
      "layer4.0.downsample.0.weight: 3\n",
      "\n",
      "1\n",
      "layer4.0.downsample.1.weight: 0\n",
      "\n",
      "1\n",
      "layer4.0.downsample.1.bias: 0\n",
      "\n",
      "1\n",
      "layer4.1.conv1.weight: 0\n",
      "layer4.1.conv2.weight: 1\n",
      "\n",
      "2\n",
      "layer4.1.conv1.weight: 2\n",
      "\n",
      "1\n",
      "layer4.1.conv1.weight: 3\n",
      "\n",
      "1\n",
      "layer4.1.bn1.weight: 0\n",
      "\n",
      "1\n",
      "layer4.1.bn1.bias: 0\n",
      "\n",
      "1\n",
      "layer4.1.conv2.weight: 2\n",
      "\n",
      "1\n",
      "layer4.1.conv2.weight: 3\n",
      "\n",
      "1\n",
      "layer4.1.bn2.weight: 0\n",
      "\n",
      "1\n",
      "layer4.1.bn2.bias: 0\n",
      "\n",
      "1\n",
      "fc.weight: 0\n",
      "fc.bias: 0\n",
      "\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "for group in groups:\n",
    "    print(group)\n",
    "    print(len(group.params))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hse_diploma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
