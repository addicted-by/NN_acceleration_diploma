Automatically generated by Mendeley Desktop 1.19.8
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop


@InProceedings{Solodskikh_2023_CVPR,
    author    = {Solodskikh, Kirill and Kurbanov, Azim and Aydarkhanov, Ruslan and Zhelavskaya, Irina and Parfenov, Yury and Song, Dehua and Lefkimmiatis, Stamatios},
    title     = {Integral Neural Networks},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2023},
    pages     = {16113-16122}
}

@misc{zhu2017prune,
      title={To prune, or not to prune: exploring the efficacy of pruning for model compression}, 
      author={Michael Zhu and Suyog Gupta},
      year={2017},
      eprint={1710.01878},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{dong2017learning,
      title={Learning to Prune Deep Neural Networks via Layer-wise Optimal Brain Surgeon}, 
      author={Xin Dong and Shangyu Chen and Sinno Jialin Pan},
      year={2017},
      eprint={1705.07565},
      archivePrefix={arXiv},
      primaryClass={cs.NE}
}

@misc{frantar2023optimal,
      title={Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning}, 
      author={Elias Frantar and Sidak Pal Singh and Dan Alistarh},
      year={2023},
      eprint={2208.11580},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{chen2023going,
      title={Going Beyond Neural Network Feature Similarity: The Network Feature Complexity and Its Interpretation Using Category Theory}, 
      author={Yiting Chen and Zhanpeng Zhou and Junchi Yan},
      year={2023},
      eprint={2310.06756},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{yu2021hessianaware,
      title={Hessian-Aware Pruning and Optimal Neural Implant}, 
      author={Shixing Yu and Zhewei Yao and Amir Gholami and Zhen Dong and Sehoon Kim and Michael W Mahoney and Kurt Keutzer},
      year={2021},
      eprint={2101.08940},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}


@Article{rs15123144,
AUTHOR = {Zhang, Chaoyan and Li, Cheng and Guo, Baolong and Liao, Nannan},
TITLE = {Neural Network Compression via Low Frequency Preference},
JOURNAL = {Remote Sensing},
VOLUME = {15},
YEAR = {2023},
NUMBER = {12},
ARTICLE-NUMBER = {3144},
URL = {https://www.mdpi.com/2072-4292/15/12/3144},
ISSN = {2072-4292},
ABSTRACT = {Network pruning has been widely used in model compression techniques, and offers a promising prospect for deploying models on devices with limited resources. Nevertheless, existing pruning methods merely consider the importance of feature maps and filters in the spatial domain. In this paper, we re-consider the model characteristics and propose a novel filter pruning method that corresponds to the human visual system, termed Low Frequency Preference (LFP), in the frequency domain. It is essentially an indicator that determines the importance of a filter based on the relative low-frequency components across channels, which can be intuitively understood as a measurement of the &ldquo;low-frequency components&rdquo;. When the feature map of a filter has more low-frequency components than the other feature maps, it is considered more crucial and should be preserved during the pruning process. We conduct the proposed LFP on three different scales of datasets through several models and achieve superior performances. The experimental results obtained on the CIFAR datasets and ImageNet dataset demonstrate that our method significantly reduces the model size and FLOPs. The results on the UC Merced dataset show that our approach is also significant for remote sensing image classification.},
DOI = {10.3390/rs15123144}
}