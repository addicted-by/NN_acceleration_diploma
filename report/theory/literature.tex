\subsection{Обзор литературы}

\subsubsection{Метод оценки важности атомарных единиц нейронных сетей}
\par
Рассмотреть статьи \cite{yu2021hessianaware} \cite{zhu2017prune} \cite{chen2023going} \cite{frantar2023optimal} \cite{dong2017learning}.
Основные проблемы таких подходов в равномерности прунинга, если он локальный, и отсутствии понимания емкости моделей, если прунинг глобальный. На практике чаще всего в первую очередь происходит профайлинг, чтобы наиболее эффективно ускорить нейронную сеть на конкретном девайсе (мобильные устройства: телефоны, часы и прочее, видеокарта). Понимая емкость каждого слоя совместно с временем его исполнения становится возможно построить наиболее эффективную (с точки зрения ускорения и сохранения качества) финальную модель. Так же проблемой является то, что все эти методы, даже второго порядка и с аналатическими оценками сохраненных параметров, требуют дообучения, чтобы вернуться к качеству оригинальной модели. Потерянная емкость способна приводить к деградациям, требующим более сложного процесса обучения, чем для модели с изначальной емкостью (для задач, связанных с генерацией изображений: denoising, super-resolution, тд, возможны смещения по цвету, регулярности, например, из-за потери емкости ConvTranspose рис. и другие).

\subsubsection{Динамический ресемплинг в непрерывных представлениях нейронных сетей}

\par
Рассмотреть статьи \cite{rs15123144} \cite{Solodskikh_2023_CVPR}. Непрерывное представление дает возможность сразу менять число атомарных единиц нейронных сетей (нейронов, фильтров, голов). Кроме этого, семплирование может быть рассмотрено как дискретизация сигнала, что связывает этот процесс с теоремой Найквиста-Шеннона-Котельникова. Эта связь может оказаться полезной для определения оптимальной частоты дискретизации как оценки емкости модели нейронной сети. Приведенные гипотезы опробованы на двух основных задачах: классификация изображений (модели: ResNet18, ViT, OmniVec, датасеты: ImageNet, CIFAR-10), суперрезолюция (модели: EDSR, SRCNN, HAT-L, датасеты: Set5, B100)